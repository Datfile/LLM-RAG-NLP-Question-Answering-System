{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM RAG NLP Project: Document-Based Question Answering System\n",
    "\n",
    "This notebook combines Large Language Models (LLM), Retrieval-Augmented Generation (RAG), and Natural Language Processing (NLP) techniques to create a document-based question answering system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers sentence-transformers faiss-cpu python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    def __init__(self):\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.vector_db = None\n",
    "        self.doc_chunks = []\n",
    "        self.chunk_embeddings = []\n",
    "        \n",
    "    def chunk_document(self, text: str, chunk_size: int = 500, chunk_overlap: int = 100) -> List[str]:\n",
    "        \"\"\"Split document into overlapping chunks\"\"\"\n",
    "        words = re.split(r'\\s+', text)\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(words):\n",
    "            end = min(start + chunk_size, len(words))\n",
    "            chunk = ' '.join(words[start:end])\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "            if end == len(words):\n",
    "                break\n",
    "                \n",
    "            start = end - chunk_overlap\n",
    "            \n",
    "        return chunks\n",
    "    \n",
    "    def create_embeddings(self, chunks: List[str]) -> None:\n",
    "        \"\"\"Generate embeddings for document chunks\"\"\"\n",
    "        self.doc_chunks = chunks\n",
    "        self.chunk_embeddings = self.embedding_model.encode(chunks, show_progress_bar=True)\n",
    "        \n",
    "        # Create FAISS index\n",
    "        dimension = self.chunk_embeddings.shape[1]\n",
    "        self.vector_db = faiss.IndexFlatL2(dimension)\n",
    "        self.vector_db.add(self.chunk_embeddings)\n",
    "    \n",
    "    def retrieve_relevant_chunks(self, query: str, k: int = 3) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Retrieve top-k most relevant document chunks for a query\"\"\"\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        distances, indices = self.vector_db.search(query_embedding, k)\n",
    "        \n",
    "        results = []\n",
    "        for idx, distance in zip(indices[0], distances[0]):\n",
    "            if idx >= 0:  # -1 indicates no result\n",
    "                results.append((self.doc_chunks[idx], float(distance)))\n",
    "                \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA System Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QASystem:\n",
    "    def __init__(self):\n",
    "        self.llm = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=\"gpt2\",  # Replace with \"gpt-3.5-turbo\" if you have OpenAI API access\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        self.doc_processor = DocumentProcessor()\n",
    "    \n",
    "    def process_document(self, document_text: str) -> None:\n",
    "        \"\"\"Process and store document for future queries\"\"\"\n",
    "        chunks = self.doc_processor.chunk_document(document_text)\n",
    "        self.doc_processor.create_embeddings(chunks)\n",
    "    \n",
    "    def answer_question(self, question: str) -> str:\n",
    "        \"\"\"Answer question based on processed documents\"\"\"\n",
    "        if not self.doc_processor.doc_chunks:\n",
    "            return \"Please upload and process a document first.\"\n",
    "            \n",
    "        # Retrieve relevant context\n",
    "        relevant_chunks = self.doc_processor.retrieve_relevant_chunks(question)\n",
    "        context = \"\\n\\n\".join([chunk for chunk, _ in relevant_chunks])\n",
    "        \n",
    "        # Generate answer using LLM\n",
    "        prompt = f\"\"\"Based on the following context, answer the question. If the answer isn't in the context, say you don't know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        response = self.llm(\n",
    "            prompt,\n",
    "            max_length=200,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )\n",
    "        \n",
    "        return response[0]['generated_text'].replace(prompt, \"\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive UI with IPython Widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize QA System\n",
    "qa_system = QASystem()\n",
    "\n",
    "# Create widgets\n",
    "document_upload = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Paste your document text here or upload a file below',\n",
    "    description='Document:',\n",
    "    layout={'width': '90%', 'height': '200px'}\n",
    ")\n",
    "\n",
    "file_upload = widgets.FileUpload(\n",
    "    description='Upload file:',\n",
    "    multiple=False\n",
    ")\n",
    "\n",
    "process_btn = widgets.Button(description=\"Process Document\")\n",
    "\n",
    "question_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your question here',\n",
    "    description='Question:',\n",
    "    layout={'width': '90%'}\n",
    ")\n",
    "\n",
    "ask_btn = widgets.Button(description=\"Ask Question\")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Event handlers\n",
    "def on_process_clicked(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        document_text = document_upload.value\n",
    "        \n",
    "        if file_upload.value:\n",
    "            # If file uploaded, use that instead\n",
    "            uploaded_file = list(file_upload.value.values())[0]\n",
    "            document_text = uploaded_file['content'].decode('utf-8')\n",
    "            \n",
    "        if not document_text.strip():\n",
    "            print(\"Please provide document text or upload a file.\")\n",
    "            return\n",
    "            \n",
    "        print(\"Processing document...\")\n",
    "        qa_system.process_document(document_text)\n",
    "        print(\"Document processed successfully!\")\n",
    "\n",
    "def on_ask_clicked(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        question = question_input.value\n",
    "        if not question.strip():\n",
    "            print(\"Please enter a question.\")\n",
    "            return\n",
    "            \n",
    "        print(\"Generating answer...\")\n",
    "        answer = qa_system.answer_question(question)\n",
    "        display(Markdown(f\"**Question:** {question}\"))\n",
    "        display(Markdown(f\"**Answer:** {answer}\"))\n",
    "\n",
    "# Attach event handlers\n",
    "process_btn.on_click(on_process_clicked)\n",
    "ask_btn.on_click(on_ask_clicked)\n",
    "\n",
    "# Display the UI\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h2>Document-Based QA System with RAG</h2>\"),\n",
    "    widgets.HTML(\"<p>Upload a document and ask questions about its content</p>\"),\n",
    "    document_upload,\n",
    "    file_upload,\n",
    "    process_btn,\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    question_input,\n",
    "    ask_btn,\n",
    "    output_area\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "1. Paste your document text in the text area OR upload a text file\n",
    "2. Click \"Process Document\"\n",
    "3. Enter your question in the question field\n",
    "4. Click \"Ask Question\"\n",
    "\n",
    "Example questions you can try (after processing a document):\n",
    "- \"What are the main points?\"\n",
    "- \"Can you summarize this document?\"\n",
    "- \"What methodology was used?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Document for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_document = \"\"\"\n",
    "Large Language Models (LLMs) are a type of artificial intelligence that has revolutionized natural language processing. \n",
    "These models are trained on vast amounts of text data and can generate human-like text, answer questions, and perform \n",
    "various language tasks. The most advanced LLMs today have hundreds of billions of parameters.\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a technique that combines the power of LLMs with external knowledge retrieval. \n",
    "When answering a question, the system first retrieves relevant documents or passages, then uses the LLM to generate \n",
    "an answer based on this context. This approach reduces hallucinations and improves answer accuracy.\n",
    "\n",
    "Key benefits of RAG include:\n",
    "1. Access to up-to-date information (since the knowledge base can be updated separately from the model)\n",
    "2. Better traceability (you can see which documents were used to generate the answer)\n",
    "3. Reduced training costs (you don't need to retrain the model when knowledge updates)\n",
    "\n",
    "Current challenges with RAG systems include:\n",
    "- Retrieval accuracy (finding the most relevant passages)\n",
    "- Integration of multiple knowledge sources\n",
    "- Handling contradictory information in the knowledge base\n",
    "\"\"\"\n",
    "\n",
    "# Preload the sample document\n",
    "document_upload.value = sample_document"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}